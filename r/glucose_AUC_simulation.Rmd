---
title: "Glucose tolerance test simulation"
author: "Jeff Walker"
date: "1/3/2020"
output: html_document
---

# Simulation
## Response variables - make this into a table
area methods

1. auc -- a weighted mean, where weights are a function of the time on either side of the point. Biased effects simply because baseline is part of area. Smaller baseline > more area
2. iauc -- analogous to change score, biased effects. Bigger baseline > more area. Increase 
3. area-cov -- weighted mean but no baseline bias. Should increase precision and power. area based on whole area or just post-baseline-area?

mean methods - easy to implement in any stats program.
4. mean - all post-baseline time points equally weighted. unconditionally unbiased. conditionally biased because of correlation between time points
5. change score -- all post-baseline time points equally weighted. should increase precision and power but at cost of bias.
6. mean-cov -- all post-baseline time points equally weighted. increase precision and power.

multivariate -- need R or scripting
7. multiple t
8. multiple t with bonferroni
9. two way ANOVA
10. lmm/repeated measures ANOVA
11. clda -- models correlation. Equivalent to mean-cov with only one post-baseline time but conditionally biased with multiple.
12. lda-cov -- models correlations. 
13. Roast -- 
14. O'Brien --

## simulations
Treatment can have a direct and indirect effect. Direct is differential effect of glucose tolerance test. Indirect is effect at post-baseline time due to treatment effect at baseline and non-independent measures (correlated error).

1. glucose tolerance effect at times 15, 30, 60
   - effects at 15, 30, 60
   - rho_models: 1) low baseline (0.2 max) + low non-baseline (0.3 max)
                 2) low baseline (0.2 max) + high non-baseline (0.8 max)
                 3) mod baseline (0.5 max) + high non-baseline (0.8 max)
   - cohen_models of direct effect: 0, 0.8, 2
2. effect of Rho
   - rho_models: 1) low baseline (0.2 max) + low non-baseline (0.3 max)
                 2) low baseline (0.2 max) + high non-baseline (0.8 max)
   - cohen_models of direct effect: 0
3. glucose tolerance effect at times 15 vs. 60 to show consequence of area as weighted mean
   - run at rho=.8 and power = 2
4. glucose tolerance effect at time 15, 30, 60 plus strain (genotype or treatment) effect at time 0 to show consequence of direct v total effect
   - run at combos of rho = 0.8
   - run at cohen glucose tolerance = 0, 2
   - for cohen_gt = 0, set cohen to 2 and alpha_prime = 0.4, 0, 0, 0, 0
   - for cohen_gt = 2, set coehn to 2 and alpha_prime = 0.4, 1, 1, 1, 0
5. (a) glucose tolerance effect of opposite directions at time 15, 30 AND
(b) treatment effect at time 0 but no glucose tolerance effect, both to show consequence of rmanova interaction p and treatment p
   - run at combos of rho = 0.8
   - for (b) cohen_gt = 0, set cohen to 2 and alpha_prime = 0.4, 0, 0, 0, 0
   - for (a) cohen_gt = 2, set coehn to 2 and alpha_prime = 0, 1, -1, 0, 0
   - run rmanova.i, rmanova.t, mean-cov, lda-cov

notes
1. what is excess type I for peeking at data and choosing a response summary measure based on pattern, for example, increased glucose at mid-time but not end-time? Better to fit a curve (cubic spline?) and get CI of curve and compare these.
2. how does conditional p-value respond to 1 vs 2 vs many post-baseline measures? does it "average out"?
3. How is performance a function of number of post-baseline measures?

notes with real data
1. COR t0, t1 from negative to small to moderate
2. scott Fig 1c 

# setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(readxl)
library(janitor)
library(data.table)
library(Hmisc) # auc using trap.rule, probably faster than my code
library(nlme)
library(afex)
library(lmerTest)
library(emmeans)
library(limma) # Roast
library(mediation)
library(multcomp)
library(ggpubr)
library(ggsci)
library(DHARMa)
library(mvtnorm)
library(doBy)

here <- here::here
data_path <- "data"
output_path <- "output"

source_folder <- "R"
auc_path <- here(source_folder, "auc.R")
source(auc_path)
```

## Sigma -- conditional error matrix

Sigma is the conditional error matrix - conditional on treatment so the correlations arise because of clustered (repeated measures) data -- each mouse is measured at multiple time points. Rho is associated correlation matrix. The exploratory simulations used an empirical sigma computed from the "Perinatal lead (Pb)" mouse paper. More empirical patterns were explored in AUC_empirical_examples.Rmd. The new simulations generate a fake Sigma with a pattern similar to the empirical patterns.

### Empirical Sigma from Perinatal lead (Pb) mouse data
#### Import Perinatal lead (Pb) mouse data
```{r import-mouse}
mouse_folder <- "Perinatal lead (Pb) exposure results in sex-specific effects on food intake, fat, weight, and insulin response across the murine life-course"
mouse_file <- "Glucose_Tolerance_Test_CompleteData.xlsx"
mouse_path <- here(data_path, mouse_folder, mouse_file)
mouse_wide <- read_excel(mouse_path,
                    sheet = "Sheet1",
                    range = "A1:Z109") %>%
  clean_names() %>%
  data.table()

times <- c(0, 15, 30, 60, 120)
glucose_cols = paste0("glucose", times)
insulin_cols = paste0("insulin", times)
mouse <- melt(mouse_wide, 
              id.vars = c("id", "sex", "exposure", "glucose_auc", "insulin_auc", "homair_liv"),
              measure.vars = list(glucose_cols, insulin_cols),
              variable.name = "time",
              value.name = c("glucose", "insulin"))
mouse[, time:=factor(glucose_cols[time], glucose_cols)]
```

#### Perinatal lead (Pb) Sigma-hat using manual conditioning

```{r}
glucose_cols <- paste0("glucose", times)
#cor(mouse_wide[sex == "m" & exposure == "16 ppm", .SD, .SDcols=glucose_cols])

m1 <- lm(glucose0 ~ sex*exposure, data=mouse_wide, na.action = "na.exclude")
mouse_wide[, glucose_cen_0 := residuals(m1)]
m1 <- lm(glucose15 ~ sex*exposure, data=mouse_wide, na.action = "na.exclude")
mouse_wide[, glucose_cen_15 := residuals(m1)]
m1 <- lm(glucose30 ~ sex*exposure, data=mouse_wide, na.action = "na.exclude")
mouse_wide[, glucose_cen_30 := residuals(m1)]
m1 <- lm(glucose60 ~ sex*exposure, data=mouse_wide, na.action = "na.exclude")
mouse_wide[, glucose_cen_60 := residuals(m1)]
m1 <- lm(glucose120 ~ sex*exposure, data=mouse_wide, na.action = "na.exclude")
mouse_wide[, glucose_cen_120 := residuals(m1)]

glucose_cen_cols <- paste0("glucose_cen_", times)
#cor(mouse_wide[sex == "m" & exposure == "16 ppm", .SD, .SDcols=glucose_cen_cols])

round(cor(mouse_wide[, .SD, .SDcols=glucose_cen_cols]),3)

```

#### Sigma-hat using gls
```{r}
fit <- gls(glucose ~ time*sex*exposure,
           data = mouse,
           weights = varIdent(form= ~ 1 | time),
           correlation= corSymm(form=~ 1| id)
)
round(cov2cor(getVarCov(fit, individual=mouse[1,id])), 3)

```

# simulation functions

Rho is the conditional error correlatio matrix.
```{r simulation functions}
  rho.1.2 <- 0.5
  rho.1.p <- 0.6
  rho.max.max <- 0.8
  rho.max.min <- 0.7
  
fake_Rho <- function(p=5,
                     rho.base.2 = 0.6,
                     rho.base.p = 0.5,
                     rho.max.max = 0.8,
                     rho.max.min = 0.7,
                     rho.min = 0.5){
  # rho.base.2 and rho.base.p control the correlations of baseline with post-baseline measures. These tend to be lower than the correlations among post-baseline measures, generally between 0 and 0.5
  # rho.max.max and rho.max.min control the maximum post-baseline correlations. In general the correlations are highest beteween succeesive times and are highest between final two times, which is rho.max.max. rho.max.min is the correlation between time 2 and 3. The correlations drop to rho.min.
  
  Rho_fake <- matrix(1, nrow=p, ncol=p)
  for(i in 1:(p-1)){
    cells <- p - i
    row.max <- (cells-1)/(p-2-1)*rho.max.min + (1-(cells-1)/(p-2-1))*rho.max.max
    inc <- -(row.max - rho.min)/(p - 2 -1)
    for(j in (i+1):p){
      if(i==1){
        Rho_fake[i,j] <- (j-2)/(p-2)*rho.base.p + (1-(j-2)/(p-2))*rho.base.2
        Rho_fake[j,i] <- Rho_fake[i,j]
      }else{
        Rho_fake[i,j] <- row.max + inc*(cells - (p-j+1))
        Rho_fake[j,i] <- Rho_fake[i,j]
      }
    }
  }
  return(Rho_fake)
}

simulate_it <- function(n=5, n2=NULL, mu, beta, alpha, Sigma, times, niter=1000, method_list){
  if(is.null(n2)){n2 <- n}
  p_cols <- paste0(method_list, "_p")
  sim_stats_cols <- c("d_baseline",
                      "r.baseline",
                      "abs.r.baseline",
                      "r.nonbaseline",
                      "abs.r.nonbaseline",
                      p_cols)
  sim_stats <- matrix(NA, nrow=niter, ncol=length(sim_stats_cols))
  colnames(sim_stats) <- sim_stats_cols
  n.times <- length(times)
  p.clda <- n.times*2 - 2

  # change names of mu (which will change colnames of Y)
  names(mu) <- paste0("glucose", times)
  
  # for Roast and Obrien
  xcols <- c("treatment", "glucose0")
  zcols <- "treatment"
  roast_form <- formula(paste('~',paste(xcols,collapse='+'),sep=''))
  
  # init fake data matrix
  fd <- data.table(treatment=factor(rep(c("cn", "tr"), c(n,n2))))
  fd[, id:=factor(1:.N)]
  
  for(iter in 1:niter){
    Y <- rbind(rmvnorm(n, mu, Sigma),
               rmvnorm(n2, mu+beta+alpha, Sigma))
    fd[, glucose_0 := Y[,1]]
    #fd[, area := apply(Y, 1, auc, x=times)]
    fd[, area := apply(Y, 1, trap.rule, x=times)] # faster because compiled?
    #fd[, area_base := apply(Y, 1, auc, x=times, baseline=TRUE)]
    fd[, area_base := apply(Y-Y[,1], 1, trap.rule, x=times)]
    fd[, glucose_mean := apply(Y[,-1], 1, mean)]
    fd[, glucose_change := glucose_mean - glucose_0]
    
    fd_wide <- cbind(fd, Y)
    fd_long <- melt(fd_wide, id.vars=c("treatment", "id", "glucose_0"),
                      measure.vars = paste0("glucose", times),
                      variable.name = "time",
                      value.name = "glucose")
    fd_long[, time:=factor(time)]
    
    # get sample correlation
    fit <- gls(glucose ~ time*treatment,
               data = fd_long,
               weights = varIdent(form= ~ 1 | time),
               correlation= corSymm(form=~ 1| id)
    )
    R.sample <- cov2cor(getVarCov(fit, individual=fd_long[1,id]))
    sim_stats[iter, "r.baseline"] <- mean(R.sample[2:n.times,1])
    sim_stats[iter, "abs.r.baseline"] <- mean(abs(R.sample[2:n.times,1]))
    R.nonbaseline <- R.sample[2:n.times, 2:n.times]
    sim_stats[iter, "r.nonbaseline"] <- mean(R.nonbaseline[lower.tri(R.nonbaseline)])
    sim_stats[iter, "abs.r.nonbaseline"] <- mean(abs(R.nonbaseline[lower.tri(R.nonbaseline)]))

    sim_stats[iter, "d_baseline"] <- mean(Y[1:n, 1]) - mean(Y[(n+1):(2*n2), 1])
    
    # p-values
    if("lm_area" %in% method_list){
      m1 <- lm(area ~ treatment, data=fd)
      sim_stats[iter, "lm_area_p"] <- coef(summary(m1))["treatmenttr","Pr(>|t|)"]
      #sim_stats[iter, "lm_area_b"] <- coef(m1)["treatmenttr"]
    }
    if("lm_base" %in% method_list){
      m2 <- lm(area_base ~ treatment, data=fd)
      sim_stats[iter, "lm_base_p"] <- coef(summary(m2))["treatmenttr","Pr(>|t|)"]
      #sim_stats[iter, "lm_base_b"] <- coef(m2)["treatmenttr"]
    }
    if("lm_cov" %in% method_list){
      m3 <- lm(area ~ treatment + glucose_0, data=fd)
      sim_stats[iter, "lm_cov_p"] <- coef(summary(m3))["treatmenttr","Pr(>|t|)"]
      #sim_stats[iter, "lm_cov_b"] <- coef(m3)["treatmenttr"]
    }
    if("lm_mean" %in% method_list){
      m4 <- lm(glucose_mean ~ treatment, data=fd)
      sim_stats[iter, "lm_mean_p"] <- coef(summary(m4))["treatmenttr","Pr(>|t|)"]
      #sim_stats[iter, "lm_mean_b"] <- coef(m4)["treatmenttr"]
    }
    if("lm_mean_cov" %in% method_list){
      m5 <- lm(glucose_mean ~ treatment + glucose_0, data=fd)
      sim_stats[iter, "lm_mean_cov_p"] <- coef(summary(m5))["treatmenttr","Pr(>|t|)"]
      #sim_stats[iter, "lm_mean_cov_b"] <- coef(m5)["treatmenttr"]
    }
    if("lm_mean_cov_alpha" %in% method_list){
      m5a <- lm(glucose_mean ~ treatment, data=fd)
      m5b <- lm(glucose_mean ~ treatment + glucose_0, data=fd)
      m5a_table <- coef(summary(m5a))
      m5b_table <- coef(summary(m5b))
      b_alpha_1 <- m5a_table["treatmenttr", "Estimate"]
      b_beta_2 <- m5b_table["treatmenttr", "Estimate"]
      se_b_alpha_1 <- m5a_table["treatmenttr", "Std. Error"]
      se_b_beta_2 <- m5b_table["treatmenttr", "Std. Error"]
      z_SED <- sqrt(se_b_alpha_1^2 + se_b_beta_2^2)
      z <- (b_beta_2 - b_alpha_1)/z_SED
      p_z <- (1-(pnorm(abs(z))))*2 # 2-tailed z-test for large sample
      
      sim_stats[iter, "lm_mean_cov_alpha_p"] <- p_z
      #sim_stats[iter, "lm_mean_cov_b"] <- coef(m5)["treatmenttr"]
    }
    if("lm_mean_change" %in% method_list){
      m6 <- lm(glucose_change ~ treatment, data=fd)
      sim_stats[iter, "lm_mean_change_p"] <- coef(summary(m6))["treatmenttr","Pr(>|t|)"]
      #sim_stats[iter, "lm_mean_change_b"] <- coef(m6)["treatmenttr"]
    }
    if("rmanova.lmm" %in% method_list ){
      m7b <- lmer(glucose ~ time*treatment + (1|id),
                  data=fd_long)
      
      sim_stats[iter, "rmanova.lmm_p"] <- anova(m7b)["time:treatment", "Pr(>F)"]
    }
    if("rmanova" %in% method_list |
       "rmanova.i" %in% method_list |
       "rmanova.t" %in% method_list){
      # rmanova returns the smallest unadjusted p of the pairs
      # rmanova.i returns the p for the anova interaction
      # rmanova.t returns the p for the anova treatment
      m7 <- aov_4(glucose ~ time*treatment + (time|id),
                  data=fd_long)
      if("rmanova" %in% method_list){
        m7.emm <- emmeans(m7,  ~ treatment*time)
        sim_stats[iter, "rmanova_p"] <- min(summary(emmeans::contrast(m7.emm, 
                 method = "revpairwise",
                 simple = "each",
                 combine = TRUE,
                 adjust = "none"))[2:n.times, "p.value"])
      }
      if("rmanova.i" %in% method_list){
        sim_stats[iter, "rmanova.i_p"] <- m7$anova_table["treatment:time", "Pr(>F)"]}
      if("rmanova.t" %in% method_list){
        sim_stats[iter, "rmanova.t_p"] <- m7$anova_table["treatment", "Pr(>F)"]}
    }
    if("multi_t" %in% method_list){
      Yy <- Y[, -1]
      fit <- lm(Yy ~ treatment, data=fd_wide)
      sim_stats[iter, "multi_t_p"] <- min(
        coefficients(summary(fit))[[1]]["treatmenttr","Pr(>|t|)"],
        coefficients(summary(fit))[[2]]["treatmenttr","Pr(>|t|)"],
        coefficients(summary(fit))[[3]]["treatmenttr","Pr(>|t|)"],
        coefficients(summary(fit))[[4]]["treatmenttr","Pr(>|t|)"]
      )
      #sim_stats[iter, "multi_t_b"] <- NA
    }
    if("clda" %in% method_list){
      # clda
#      design <- model.matrix( ~ time + treatment:time, data=fd_long)
      # remove intercept column and effect of tr at time 0
#      X <- design[, -c(1, which(colnames(design) == "timeglucose0:treatmenttr"))]

            # first check equivalence with single post baseline time
      check_equivalence <- FALSE
      if(check_equivalence == TRUE){
        inc <- which(fd_long$time=="glucose0" | fd_long$time=="glucose15")
        X.check <- design[inc, c(2,7)]
        m8a <- gls(glucose ~ X.check, # clda
                   data = fd_long[time=="glucose0" | time=="glucose15"],
                   weights = varIdent(form= ~ 1 | time),
                   correlation= corSymm(form=~ 1| id)
        )
        m8b <- lm(glucose ~ glucose_0 + treatment, # ancova-like
                  data = fd_long[time=="glucose15"]
        )
        coef(summary(m8a))
        coef(summary(m8b))
        # note that clda has smaller SE.
        # from https://datascienceplus.com/taking-the-baseline-measurement-into-account-constrained-lda-in-r/. By setting weights = varIdent(form = ~ 1 | Time) a separate standard deviation will be estimated for each time point and a seperate correlation will be estimated for each pair of time points (= unstructured variance covariance matrix). By setting weights = varIdent(form = ~ 1 | Time:Group), a separate variance is estimated for each combination of Group and Time (Pre-Exp Post-Exp Pre-Con Post-Con ). The argument correlation=corSymm (form = ~ 1 | Id) defines the subject levels. The correlation structure is assumed to apply only to observations within the same subject (in our example: Id); observations from different subjects (a different value for Id) are assumed to be uncorrelated.
        
      }

#      m8 <- gls(glucose ~ X,
#                 data = fd_long,
#                 weights = varIdent(form= ~ 1 | time),
#                 correlation= corSymm(form=~ 1| id))
      
      # # an alternative that gives same results. Simply code the interaction columns into a factor
      fd_clda <- copy(fd_long)
      fd_clda[, time.treatment := ifelse(time != "glucose0" & treatment=="tr", paste0(time, ":tr"), "cont")]
      fd_clda[, time.treatment := factor(time.treatment, c("cont","glucose15:tr", "glucose30:tr",  "glucose60:tr", "glucose120:tr"))]
      m8 <- gls(glucose ~ time + time.treatment,
                 data = fd_clda,
                 weights = varIdent(form= ~ 1 | time),
                 correlation= corSymm(form=~ 1| id))
      
      r_fit <- cov2cor(getVarCov(m8))
      r_bar <- mean(r_fit[upper.tri(r_fit)])
      # effective sample size
      # equation 10 Faes 2011
      # m = nN/(1 + rho(n-1))
      # rho is r_bar, N is number of IDs, n is number of measures w/in ID
      N = n + n2
      m <- round((n.times*N)/(1 + r_bar*(n.times - 1)), 0)

      #m8.z <- summary(glht(m8, matrix(c(0, 0,0,0,0,1,1,1,1), 1)))
      m8.t <- summary(glht(m8, 
                            matrix(c(0, 0,0,0,0,1,1,1,1), 1),
                            df = m - p.clda))

      sim_stats[iter, "clda_p"] <- m8.t$test$pvalues
      #sim_stats[iter, "clda_b"] <- m8.t$test$coefficients/4
    }
    if("lmm_cov" %in% method_list){ # this is same as lda_cov
      subdata <- fd_long[time != "glucose0"] # needed for emmeans
      m10 <- lme(glucose ~ time*treatment + glucose_0,
                random = ~1|id,
                data = subdata,
                weights = varIdent(form= ~ 1 | time),
                correlation= corSymm(form=~ 1 | id))
      # note interaction coefficients are not effects at times 30, 60, 120 
      # but differences in effect from that at time 15. This is *not* what we want.
      m10.emm <- emmeans(m10, 
                        specs=c("treatment"),
  #                      mode = "boot-satterthwaite",
                        mode = "df.error",
                        data = subdata)
      m10.trt <- emmeans::contrast(m10.emm, method="revpairwise")

      sim_stats[iter, "lda_cov_p"] <- summary(m10.trt)[, "p.value"]
      
    }
    if("lda_cov" %in% method_list){
      subdata <- fd_long[time != "glucose0"] # needed for emmeans
      m9 <- gls(glucose ~ time*treatment + glucose_0,
                data = subdata,
                weights = varIdent(form= ~ 1 | time),
                correlation= corSymm(form=~ 1 | id))
      # note interaction coefficients are not effects at times 30, 60, 120 
      # but differences in effect from that at time 15. This is *not* what we want.
      m9.emm <- emmeans(m9, 
                        specs=c("treatment"),
  #                      mode = "boot-satterthwaite",
                        mode = "df.error",
                        data = subdata)
      m9.trt <- emmeans::contrast(m9.emm, method="revpairwise")

      sim_stats[iter, "lda_cov_p"] <- summary(m9.trt)[, "p.value"]
      #sim_stats[iter, "lda_cov_b"] <- summary(m9.trt)[, "estimate"]
    }
    if("roast" %in% method_list){
      Yt <- t(Y[, -1]) # responses in rows
      design <- model.matrix(roast_form, data=fd_wide)
      colnames(design)[1] <- 'Intercept' # change '(Intercept)' to 'Intercept'
      prob <- roast(y=Yt,design=design,contrast=2, nrot=2000)$p['UpOrDown','P.Value']
      sim_stats[iter, "roast_p"] <- roast(y=Yt,design=design,contrast=2, nrot=2000)$p['UpOrDown','P.Value']
      #sim_stats[iter, "roast_b"] <- NA
      # cont.matrix <- makeContrasts(delta="zhedonia-zeudaimonia",levels=design)
      # prob['delta'] <- roast(y=Yt,design=design,contrast=cont.matrix, nrot=perms)$p['UpOrDown','P.Value']
    }
    if("obrien" %in% method_list){
      Yy <- Y[, -1]
      # design matrix for obrien
      X2 <- model.matrix(~ glucose0, data=fd_wide)
      XTXI <- solve(t(X2)%*%X2)
      fit <- lm.fit(X2, Yy)
      e <- fit$residuals
      ranks <- apply(e, 2, rank)
      ranksum <- apply(ranks, 1, sum)
      obrien.p <- t.test(ranksum ~ fd_wide$treatment, var.equal=TRUE)$p.value
      # wilcox.test(ranksum ~ fd_wide$treatment)
      # rank2 <- apply(e, 1, sum)
      # wilcox.test(rank2 ~ fd_wide$treatment)
      
      sim_stats[iter, "obrien_p"] <- obrien.p
      #sim_stats[iter, "obrien_b"] <- NA
    }
  }
  return(sim_stats)
}

simulation_wrapper <- function(
  niter = 1000,
  n = 5,
  n2 = NULL,
  method_list,
  times = c(0, 15, 30, 60, 120),
  gtt_effects = c(0, 1, 1, 1, 0), # added effect of gtt
  pre_effect = 0, # effect that would occur without gtt
  mu,
  sigma,
  cohen_list = c(0, 0.8, 2),
  cor_models
){
  p <- length(times)
  combis <- expand.grid(cohen = cohen_list, cor_model = 1:nrow(cor_models))

  for(i in 1:nrow(combis)){
    cohen <- combis[i, "cohen"]
    baseline_max <- cor_models[combis[i, "cor_model"], "baseline_max"]
    non_baseline_max <- cor_models[combis[i, "cor_model"], "non_baseline_max"]
    R <- fake_Rho(p,
                  rho.base.2 = baseline_max,
                  rho.base.p = 3/4*baseline_max,
                  rho.max.max = non_baseline_max,
                  rho.max.min = 7/8*non_baseline_max,
                  rho.min = 5/8*non_baseline_max)
    
    R_0 <- R[,1] # correlation of each time with time0
    R_0[1] <- 0 # don't add this component to first beta
    Sigma <- diag(sigma)%*%R%*%diag(sigma)
    
    # total = direct + indirect
    beta_1 <- R_0*sigma/sigma[1]
    beta <- gtt_effects*cohen*sigma
    alpha <- pre_effect*cohen*sigma[1]

    sim_stats <- simulate_it(n = n, 
                             mu = mu, 
                             beta = beta,
                             alpha = alpha,
                             Sigma = Sigma,
                             times = times, 
                             niter = niter,
                             method_list = method_list)
    
    res <- rbind(res, data.table(iter = 1:niter,
                                 effect = cohen,
                                 baseline_max = baseline_max,
                                 non_baseline_max = non_baseline_max,
                                 data.table(sim_stats)))
  }
  
  res[, effect := factor(effect)]
  res[, baseline_max := factor(baseline_max)]
  res[, non_baseline_max := factor(non_baseline_max)]
  return(res)
}
```

# Simulations
## global settings and parameters
```{r global}
  n <- 6 # sample per treatment level
  p = 5 # number of time periods
  niter <- 2000 # number of iterations in simulation per parameter combination
  
  # empirical response mean
  mu <- c(148, 230, 230, 200, 146) # mu of control in perinatal lead
  emp.sigma.ratio <- c(0.12, 0.3, 0.3, 0.3, 0.3) # rougly median of empirical
  emp.sigma <- emp.sigma.ratio*(mu[2]-mu[1])
```

## Simulation 1 -- glucose tolerance effect at times 15, 30, 60
glucose tolerance effect at times 15, 30, 60
   - times 0, 15, 30, 60, 120
   - rho_models: 1) low baseline (0.2 max) + low non-baseline (0.3 max)
                 2) low baseline (0.2 max) + high non-baseline (0.8 max)
                 3) mod baseline (0.5 max) + high non-baseline (0.8 max)
   - cohen_models of direct effect: 0, 0.8, 1.5

```{r simulation-1, message=FALSE, warning=FALSE}
# output fileame
ptm <- proc.time()
do_it <- TRUE
if(do_it == TRUE){
  write_it <- FALSE
  sim_id <- "simulation-1."
  file_id <- paste(sample(c(letters, LETTERS, 1:9), 5), collapse="")
  
  res <- data.table(NULL)
  method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")
  
  times <- c(0, 15, 30, 60, 120)
  gtt_effects <- c(0, 1, 1, 1, 0)
  cohen_list <- c(0, 0.8, 1.5)
  cor_models <- t(matrix(c(
    c(0.6, 0.8)),
    nrow = 2
  ))
  colnames(cor_models) <- c("baseline_max", "non_baseline_max")
  
  set.seed(1)
  res <- simulation_wrapper(
    niter = niter,
    n = n,
    n2 = NULL,
    method_list = method_list,
    times = times,
    gtt_effects = gtt_effects,
    pre_effect = 0,
    mu = mu,
    sigma = emp.sigma,
    cohen_list = cohen_list,
    cor_models = cor_models
  )
  
  if(write_it == TRUE){
    fn <- paste0(sim_id, file_id, ".Rds")
    save_file_path <- here(output_path, fn)
    saveRDS(object = res, file = save_file_path)
  }
}
sim_time <- proc.time() - ptm
```

## Simulation 1B -- effect of unequal n

```{r simulation-1b-unequal-n, warning=FALSE, message=FALSE}
# output fileame
ptm <- proc.time()
do_it <- TRUE
if(do_it == TRUE){
  write_it <- TRUE
  sim_id <- "simulation-1B."
  file_id <- paste(sample(c(letters, LETTERS, 1:9), 5), collapse="")
  
  res <- data.table(NULL)
  method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")
  
  times <- c(0, 15, 30, 60, 120)
  gtt_effects <- c(0, 1, 1, 1, 0)
  cohen_list <- c(0, 0.8, 1.5)
  cor_models <- t(matrix(c(
    c(0.6, 0.8)),
    nrow = 2
  ))
  colnames(cor_models) <- c("baseline_max", "non_baseline_max")
  
  set.seed(1)
  res <- simulation_wrapper(
    niter = niter,
    n = 8,
    n2 = 4,
    method_list = method_list,
    times = times,
    gtt_effects = gtt_effects,
    pre_effect = 0,
    mu = mu,
    sigma = emp.sigma,
    cohen_list = cohen_list,
    cor_models = cor_models
  )
  
  if(write_it == TRUE){
    fn <- paste0(sim_id, file_id, ".Rds")
    save_file_path <- here(output_path, fn)
    saveRDS(object = res, file = save_file_path)
  }
}
sim_time <- proc.time() - ptm
```

## Simulation 2 -- effect of Rho
glucose tolerance effect at times 15, 30, 60
   - times 0, 15, 30, 60, 120
   - rho_models: 1) low baseline (0.2 max) + low non-baseline (0.3 max)
                 2) low baseline (0.2 max) + high non-baseline (0.8 max)
                 3) mod baseline (0.5 max) + high non-baseline (0.8 max)
   - cohen_models of direct effect: 0

```{r simulation-2, message=FALSE, warning=FALSE}
# output fileame
do_it <- TRUE
if(do_it == TRUE){
  write_it <- TRUE
  sim_id <- "simulation-2."
  file_id <- paste(sample(c(letters, LETTERS, 1:9), 5), collapse="")
  
  res <- data.table(NULL)
  method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")
  
  times <- c(0, 15, 30, 60, 120)
  gtt_effects <- c(0, 1, 1, 1, 0)
  cohen_list <- c(0)
  cor_models <- t(matrix(c(
    c(0.2, 0.3),
    c(0.2, 0.8)),
    nrow = 2
  ))
  colnames(cor_models) <- c("baseline_max", "non_baseline_max")
  
  set.seed(2)
  res <- simulation_wrapper(
    niter = niter,
    n = n,
    n2 = NULL,
    method_list = method_list,
    times = times,
    gtt_effects = gtt_effects,
    pre_effect = 0,
    mu = mu,
    sigma = emp.sigma,
    cohen_list = cohen_list,
    cor_models = cor_models
  )
  
  if(write_it == TRUE){
    fn <- paste0(sim_id, file_id, ".Rds")
    save_file_path <- here(output_path, fn)
    saveRDS(object = res, file = save_file_path)
  }
}
```

## Simulation 3 -- effect of location of difference

glucose tolerance effect at times 15 vs. 60 to show consequence of area as weighted mean
   - run at rho=.8 and power = 2

```{r simulation-3, message=FALSE, warning=FALSE}
# output fileame
ptm <- proc.time()
do_it <- TRUE
if(do_it == TRUE){
  write_it <- TRUE
  sim_id <- "simulation-3."
  file_id <- paste(sample(c(letters, LETTERS, 1:9), 5), collapse="")
  
  res <- data.table(NULL)
  method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")
  
  times <- c(0, 15, 30, 60, 120)
  gtt_effect_model <- (matrix(c(
    c(0, 1, 0, 0, 0),
    c(0, 0, 0, 1, 0)),
    ncol = 2
  ))
  colnames(gtt_effect_model) <- c("t15", "t60")
  
  cohen_list <- c(1.5)
  cor_models <- t(matrix(c(
    c(0.6, 0.8)),
    nrow = 2
  ))
  colnames(cor_models) <- c("baseline_max", "non_baseline_max")
  
  set.seed(3)
  res15 <- simulation_wrapper(
    niter = niter,
    n = n,
    n2 = NULL,
    method_list = method_list,
    times = times,
    gtt_effects = gtt_effect_model[,1],
    pre_effect = 0,
    mu = mu,
    sigma = emp.sigma,
    cohen_list = cohen_list,
    cor_models = cor_models
  )
  res60 <- simulation_wrapper(
    niter = niter,
    n = n,
    n2 = NULL,
    method_list = method_list,
    times = times,
    gtt_effects = gtt_effect_model[,2],
    pre_effect = 0,
    mu = mu,
    sigma = emp.sigma,
    cohen_list = cohen_list,
    cor_models = cor_models
  )
  res <- rbind(data.table(effect_model="t15", res15),
               data.table(effect_model="t60", res60))
  
  
  if(write_it == TRUE){
    fn <- paste0(sim_id, file_id, ".Rds")
    save_file_path <- here(output_path, fn)
    saveRDS(object = res, file = save_file_path)
  }
}
sim_time <- proc.time() - ptm
```

## Simulation 4 -- effect of initial difference

glucose tolerance effect at time 15, 30, 60 plus strain (genotype or treatment) effect at time 0 to show consequence of direct v total effect
   - run at combos of rho = 0.8
   - run at cohen glucose tolerance = 0, 2
   - for cohen_gt = 0, set cohen to 2 and alpha_prime = 0.4, 0, 0, 0, 0
   - for cohen_gt = 2, set coehn to 2 and alpha_prime = 0.4, 1, 1, 1, 0

```{r simulation-4, message=FALSE, warning=FALSE}
# output fileame
ptm <- proc.time()
do_it <- TRUE
if(do_it == TRUE){
  write_it <- TRUE
  sim_id <- "simulation-4."
  file_id <- paste(sample(c(letters, LETTERS, 1:9), 5), collapse="")
  
  res <- data.table(NULL)

  method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")

  
  times <- c(0, 15, 30, 60, 120)
  gtt_effect_model <- c(0, 0, 0, 0, 0)
  
  cohen_list <- c(0.8)
  cor_models <- t(matrix(c(
    c(0.6, 0.8)),
    nrow = 2
  ))
  colnames(cor_models) <- c("baseline_max", "non_baseline_max")
  
  set.seed(3)
  res <- data.table(NULL)
  # res0 <- simulation_wrapper(
  #   niter = niter,
  #   n = n,
  #   n2 = NULL,
  #   method_list = method_list,
  #   times = times,
  #   gtt_effects = gtt_effect_model,
  #   pre_effect = 0,
  #   mu = mu,
  #   sigma = emp.sigma,
  #   cohen_list = cohen_list,
  #   cor_models
  # )
  # res <- rbind(res, data.table(effect_model="no pre", res0))
  res1 <- simulation_wrapper(
    niter = niter,
    n = n,
    n2 = NULL,
    method_list = method_list,
    times = times,
    gtt_effects = gtt_effect_model,
    pre_effect = 1, # this is presence/absence, cohen sets the magnitude
    mu = mu,
    sigma = emp.sigma,
    cohen_list = cohen_list,
    cor_models = cor_models
  )
  res <- rbind(res, data.table(effect_model="pre", res1))
  
  if(write_it == TRUE){
    fn <- paste0(sim_id, file_id, ".Rds")
    save_file_path <- here(output_path, fn)
    saveRDS(object = res, file = save_file_path)
  }
}
sim_time <- proc.time() - ptm
```

# Results

```{r functions}
expit <- function(x) {exp(x)/(1+exp(x))} # the inverse logit function. This generates the probability of the event p
logit <- function(p) {log(p/(1-p))} # the log of the odds or "logodds" given the probability of an event p. This is NOT the odds ratio, which is the ratio of two odds.
```

```{r summary functions}
melt_it <- function(res, method_list){
  p_cols <- paste0(method_list, "_p")
  if("effect_model" %nin% names(res)){ #%nin% from Hmisc
    res[, effect_model := "not.app."]
  }
  if("balanced" %nin% names(res)){ #%nin% from Hmisc
    res[, balanced := "not.app."]
  }
  res_long <- melt(res, 
                    id.vars=c("iter", "effect_model", "effect", "balanced", "baseline_max", "non_baseline_max", "d_baseline", "r.baseline", "abs.r.baseline", "r.nonbaseline", "abs.r.nonbaseline"),
                    measure.vars = p_cols,
                    variable.name = "method",
                    value.name = c("p"))
  res_long[, method := factor(method_list[method], method_list)]
  res_long[, significant := ifelse(p < 0.05, 1, 0)]
  return(res_long)
}

sim_summary <- function(res_long){
  p_summary <- res_long[!is.na(p), .(freq_lt_05 = sum(p < 0.05)/max(iter)),
          by=.(effect, baseline_max, non_baseline_max, method, effect_model, balanced)]
  return(p_summary)
}

```

Researchers most commonly test for a treatment effect on a glucose tolerance curve by either comparing treatment responses at each time point separately, or comparing an area under curve (AUC) composite measure in a single test. Here, I show that a simple comparison of the mean of the post-baseline values with the baseline value as a covariate

1. is a more powerful test. That is, it takes fewer samples to provide sufficient evidence of the direction of the treatment effect.
2. has the expected Type I error rate, while that of the comparison of individual time points is inflated.
3. has the expected Type I error rate conditional on the difference in baseline between treatments, while that of the comparison of AUC increases with the magnitude of the baseline difference.

# methods - make this into a table
area methods
1. auc -- a weighted mean, where weights are a function of the time on either side of the point. Biased effects simply because baseline is part of area. Smaller baseline > more area
2. iauc -- analogous to change score, biased effects. Bigger baseline > more area. Increase 
3. area-cov -- weighted mean but no baseline bias. Should increase precision and power.

mean methods - easy to implement in any stats program.
4. mean - all time points equally weighted. unconditionally unbiased. conditionally biased because of correlation between time points
5. change score -- all time points equally weighted. should increase precision and power but at cost of bias.
6. mean-cov -- all time points equally weighted. increase precision and power.

multivariate -- need R or scripting
7. clda -- models correlation. Equivalent to mean-cov with only one post-baseline time but conditionally biased with multiple.
8. lda-cov -- models correlations. 
9. Roast -- 
10. O'Brien --

# Results

## simulations
1. glucose tolerance effect at times 15, 30, 60
   - run at combos of rho = .3, .8 and cohen = 0, 0.8, 2
2. glucose tolerance effect at times 15 vs. 60 to show consequence of area as weighted mean
   - run at rho=.8 and power = 2
3. glucose tolerance effect at time 15, 30, 60 plus strain effect at time 0 and 120 to show consequence of direct v total effect
   - run at combos of rho = 0.8
   - run at cohen glucose tolerance = 0, 2
   - for cohen_gt = 0, set cohen to 2 and alpha_prime = 0.4, 0, 0, 0, 0
   - for cohen_gt = 2, set coehn to 2 and alpha_prime = 0.4, 1, 1, 1, 0

most relevant result
1. lm-cov and roast have overall best performance: high power without inflated Type I or inflated conditional Type I.

most relevant to researchers using individual tests
2. individual tests have more false positives. If there is some reason to want the estimate at each time point then individual tests okay.

most relevant to researchers using area as response

3. area measures are weighted means. consequence is that some differences are down-weight some up-weighted. consequence on power is that lower power if differences spread across curve, really low power if differences at time 1, but high power if difference in time 3.
4. Mean response and multivariate response have high power and about equal power
5. clda has high power but at cost of high false positive
6. Area has strong conditional bias in probability, consquence that when baseline diff is large due to random sampling, inflated type I. The reason is 1) because area is a function of baseline and 2) correlation between time0 and other times. The later also effects other measures that do not explicitly model baseline value as covariate, most notably mean response. 

Other results
7. clda has high power but at cost of high type I
8. clda is conditionally biased by baseline, despite the equivalency of clda and lm-cov when there is only one post-baseline time point.

## Type I error and power (Simulation 1)
```{r simulation-1-import}
# import original n1=n2
fn <- "simulation-1.dIW3e.Rds" #
file_path <- here(output_path, fn)
sim1a <- data.table(readRDS(file_path))

# import 1b, n1 != n2
fn <- "simulation-1B.dRbev.Rds" #
file_path <- here(output_path, fn)
sim1b <- data.table(readRDS(file_path))

sim1 <- rbind(data.table(balanced="yes", sim1a),
              data.table(balanced="no", sim1b))
sim1[, balanced := factor(balanced, c("yes", "no"))]

method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")


# bonferroni rmanova and multi_t
sim1[, rmanova_p:=ifelse(rmanova_p*4 > 1, 1, rmanova_p*4)]
sim1[, multi_t_p:=ifelse(multi_t_p*4 > 1, 1, multi_t_p*4)]

sim1_long <- melt_it(sim1, method_list)

sim1_sum <- sim_summary(sim1_long)
sim1_sum[, Rho_combis:=factor(paste(baseline_max, non_baseline_max, sep=", "))]
sim1_sum[, Rho_model:=factor(as.integer(Rho_combis))]
```

### Unconditional Type I error

```{r simulation-1-unconditional-type1}
jco_pal <- pal_jco()(6)
#scales::show_col(jco_pal)

group_colors <- c(rep(jco_pal[c(2,5,4,6)],
                      c(3,3,3,4)))[1:length(method_list)]
group_shapes <- c(rep(c(15, 16, 17), 4), 18)[1:length(method_list)]
  
pd <- position_dodge(0.8)
gg1 <- ggplot(data = sim1_sum[effect == "0"],
       aes(x=1, y=freq_lt_05, color=method, shape=method)) +
  geom_hline(yintercept = 0.05, linetype = "dashed", alpha=0.5) +
  geom_point(position = pd) +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  facet_grid(. ~ balanced, labeller = "label_both") +
  ylab("Frequency p < 0.05") +
  xlab("") +
  theme_pubr() +
  theme(legend.position="bottom",
        axis.text.x = element_blank()) +
  guides(col = guide_legend(ncol = 5, byrow = FALSE)) +
  NULL
gg1
```

### Type I error conditional on false initial difference

```{r simulation-1-conditional}
jco_pal1 <- pal_jco()(6)
group_colors <- c(rep(jco_pal[c(2,5,4,6)],
                      c(3,3,3,4)))[1:length(method_list)]
group_shapes <- c(rep(c(15, 16, 17), 4), 18)[1:length(method_list)]

subdata <- sim1_long[effect == "0",]
subdata[, d_baseline_std := abs(d_baseline)/emp.sigma]
subdata[, logit_p := ifelse(is.infinite(logit(p)), NA, logit(p))]
plot_data <- data.table(NULL)
new_x <- seq(0, 1.5, by=0.1) # uniform values of d_baseline_std for prediction
new_data <- data.table(d_baseline_std = new_x)
for(balanced_i in c("yes", "no")){
  for(method_i in method_list){
    m1 <- glm(significant ~ d_baseline_std, 
              family = binomial(link="logit"), 
              data=subdata[method==method_i & balanced == balanced_i])
    prob.m1 <- predict(m1, new_data, type="response")
    m2 <- lm(logit_p ~ d_baseline_std, 
             data=subdata[method==method_i])
    prob.m2 <- expit(predict(m2, new_data))
    plot_data <- rbind(plot_data, data.table(method = method_i,
                                             balanced = balanced_i,
                                             d_baseline_std = new_x, 
                                             prob.m1=prob.m1,
                                             prob.m2=prob.m2))
  }
}
plot_data[, method := factor(method, method_list)]
plot_data[, balanced := factor(balanced, c("yes", "no"))]

gg1 <- ggplot(data=plot_data,
              aes(x=d_baseline_std, y=prob.m1, color=method, shape=method)) +
  geom_hline(yintercept = 0.05, linetype = "dashed", alpha=0.5) +
  geom_point() +
  geom_line() +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  ylab("Probability of p < 0.05") +
  xlab("Difference at Time0 (in standard deviation units)") +
  theme_pubr() +
  theme(legend.position="bottom") +
  guides(col = guide_legend(ncol = 5)) +
  facet_grid(. ~ balanced, labeller = "label_both") +
  NULL
gg1

gg2 <- ggplot(data=plot_data,
              aes(x=d_baseline_std, y=prob.m2, color=method, shape=method)) +
  geom_hline(yintercept = 0.05, linetype = "dashed", alpha=0.5) +
  geom_point() +
  geom_line() +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  ylab("Probability of p < 0.05") +
  xlab("Difference at Time0 (in standard deviation units)") +
  theme_pubr() +
  theme(legend.position="bottom") +
  guides(col = guide_legend(ncol = 5)) +
  NULL
#gg2
```

### Power (simulation 1)

balanced

```{r simulation-1-power-balanced}
jco_pal <- pal_jco()(6)
#scales::show_col(jco_pal)

group_colors <- c(rep(jco_pal[c(2,5,4,6)],
                      c(3,3,3,4)))[1:length(method_list)]
group_shapes <- c(rep(c(15, 16, 17), 4), 18)[1:length(method_list)]
  
pd <- position_dodge(0.8)
gg1 <- ggplot(data = sim1_sum[effect != "0" & balanced == "yes"],
       aes(x=1, y=freq_lt_05, color=method, shape=method)) +
  geom_point(position = pd) +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  facet_grid(. ~ effect, labeller = "label_both") +
  ylab("Frequency p < 0.05") +
  xlab("") +
  theme_pubr() +
  theme(legend.position="bottom",
        axis.text.x = element_blank()) +
  guides(col = guide_legend(ncol = 5, byrow = FALSE)) +
  NULL
gg1
```

both balanced and unbalanced (supplement fig)

```{r simulation-1-power-balanced-supplement}
jco_pal <- pal_jco()(6)
#scales::show_col(jco_pal)

group_colors <- c(rep(jco_pal[c(2,5,4,6)],
                      c(3,3,3,4)))[1:length(method_list)]
group_shapes <- c(rep(c(15, 16, 17), 4), 18)[1:length(method_list)]
  
pd <- position_dodge(0.8)
gg1 <- ggplot(data = sim1_sum[effect != "0"],
       aes(x=1, y=freq_lt_05, color=method, shape=method)) +
  geom_point(position = pd) +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  facet_grid(balanced ~ effect, labeller = "label_both") +
  ylab("Frequency p < 0.05") +
  xlab("") +
  theme_pubr() +
  theme(legend.position="bottom",
        axis.text.x = element_blank()) +
  guides(col = guide_legend(ncol = 5, byrow = FALSE)) +
  NULL
gg1
```

## Effect of correlation on conditional Type I error (Simulation 2)

```{r simulation-2-import}
fn <- "simulation-2.lQHj2.Rds" #
file_path <- here(output_path, fn)
sim2 <- data.table(readRDS(file_path))

method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")

# bonferroni rmanova and multi_t
sim2[, rmanova_p:=ifelse(rmanova_p*4>1,1,rmanova_p*4)]
sim2[, multi_t_p:=ifelse(multi_t_p*4>1,1,multi_t_p*4)]
sim2_long <- melt_it(sim2, method_list)

# add sim1 data
sim2_long <- rbind(sim1_long[effect=="0" & balanced == "yes"], sim2_long)
sim2_long[, Rho_combo := paste(baseline_max, non_baseline_max, sep=", ")]
Rho_models <- unique(sim2_long$Rho_combo)
sim2_long[, R_model := factor(Rho_combo, Rho_models)]

sim2_long[, .(baseline=mean(r.baseline),
              non_baseline=mean(r.nonbaseline)),
          by = .(R_model)]
```

```{r simulation-2-conditional-by-rho}
jco_pal1 <- pal_jco()(6)
group_colors <- c(rep(jco_pal[c(2,5,4,6)],
                      c(3,3,3,4)))[1:length(method_list)]
group_shapes <- c(rep(c(15, 16, 17), 4), 18)[1:length(method_list)]

sim2_long[, d_baseline_std := abs(d_baseline)/emp.sigma]
sim2_long[, logit_p := ifelse(is.infinite(logit(p)), NA, logit(p))]
plot_data <- data.table(NULL)
new_d_baseline <- seq(0, 1.5, by=0.1) # uniform values of d_baseline_std for prediction
new_data <- data.table(d_baseline_std = new_d_baseline)
for(model_i in levels(sim2_long$R_model)){
  for(method_i in method_list){
    m1 <- glm(significant ~ d_baseline_std, 
              family = binomial(link="logit"), 
              data=sim2_long[method==method_i & R_model==model_i])
    prob.m1 <- predict(m1, new_data, type="response")
    plot_data <- rbind(plot_data, data.table(method = method_i,
                                             R_model = model_i,
                                             d_baseline_std = new_d_baseline, 
                                             prob.m1=prob.m1))
  }
}

plot_data[, method := factor(method, method_list)]
plot_data[, R_model := factor(R_model, c("0.6, 0.8", "0.2, 0.8", "0.2, 0.3"))]
gg1 <- ggplot(data=plot_data,
              aes(x=d_baseline_std, y=prob.m1, color=method, shape=method)) +
  geom_hline(yintercept = 0.05, linetype = "dashed", alpha=0.5) +
  geom_point() +
  geom_line() +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  ylab("Probability of p < 0.05") +
  xlab("Difference at Time0 (in standard deviation units)") +
  theme_pubr() +
  theme(legend.position="bottom") +
  guides(col = guide_legend(ncol = 5, byrow = FALSE)) +
  facet_grid(.~R_model, labeller = "label_both") +
  NULL
gg1


```

## Consequence of location of effect (simulation 3)

```{r fig-freq-lt-05}
jco_pal1 <- pal_jco()(6)
#scales::show_col(jco_pal)

group_colors <- c(rep(jco_pal[c(2,1,4)], each=3), jco_pal[4], jco_pal[4])[1:length(method_list)]
group_shapes <- c(rep(c(15, 16, 17), 3), 18, 19)[1:length(method_list)]
  
pd <- position_dodge(0.8)
gg1 <- ggplot(data = res_summary[time_model=="standard" & rmax=="0.8"],
       aes(x=effect_model, y=freq_lt_05, color=method, shape=method)) +
  geom_point(position = pd) +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  facet_grid(. ~ effect, scales = "free_y", labeller = "label_both") +
  ylab("Frequency p < 0.05") +
  xlab("At what time is effect present?") +
  theme_pubr() +
  theme(legend.position="bottom") +
 # guides(col = guide_legend(ncol = 3)) +
  guides(col = guide_legend(ncol = 4)) +
  NULL
gg1
```

## Consequence true initial effect on of Type I error (simulation 4)

```{r sim4-alpha}
fn <- "simulation-4.fAbh3.Rds"
file_path <- here(output_path, fn)
sim4 <- data.table(readRDS(file_path))
#sim4[, lm_mean_cov_alpha_p:=NULL]

# add simulation 1
fn <- "simulation-1.dIW3e.Rds"
file_path <- here(output_path, fn)
sim1b <- data.table(readRDS(file_path))[effect=="0"]
sim1b[, effect_model:="no pre"]

sim4 <- rbind(sim4, sim1b)
sim4[, effect_model := factor(effect_model)]

method_list <- c("lm_area", "lm_base", "lm_cov", "lm_mean", "lm_mean_change", "lm_mean_cov", "multi_t", "obrien", "roast", "rmanova", "rmanova.i", "clda", "lda_cov")


# bonferroni rmanova and multi_t
sim4[, rmanova_p:=ifelse(rmanova_p*4>1,1,rmanova_p*4)]
sim4[, multi_t_p:=ifelse(multi_t_p*4>1,1,multi_t_p*4)]

sim4_long <- melt_it(sim4, method_list)
sim4_long[!is.na(p), .(freq_lt_05 = sum(p < 0.05)/max(iter)),
          by=.(effect_model, method)]

```

```{r sim4-conditional}
jco_pal <- pal_jco()(6)
group_colors <- c(rep(jco_pal[c(2,5,4,6)],
                      c(3,3,3,4)))[1:length(method_list)]
group_shapes <- c(rep(c(15, 16, 17), 4), 18)[1:length(method_list)]

sim4_long[, d_baseline_std := abs(d_baseline)/emp.sigma]
sim4_long[, logit_p := ifelse(is.infinite(logit(p)), NA, logit(p))]
plot_data <- data.table(NULL)
new_d_baseline <- seq(0, 1.5, by=0.1) # uniform values of d_baseline_std for prediction
new_data <- data.table(d_baseline_std = new_d_baseline)
for(model_i in levels(sim4_long$effect_model)){
  for(method_i in method_list){
    m1 <- glm(significant ~ d_baseline_std, 
              family = binomial(link="logit"), 
              data=sim4_long[method==method_i & effect_model==model_i])
    prob.m1 <- predict(m1, new_data, type="response")
    plot_data <- rbind(plot_data, data.table(method = method_i,
                                             effect_model = model_i,
                                             d_baseline_std = new_d_baseline, 
                                             prob.m1=prob.m1))
  }
}

plot_data[, method := factor(method, method_list)]
gg1 <- ggplot(data=plot_data,
              aes(x=d_baseline_std, y=prob.m1, color=method, shape=method)) +
  geom_hline(yintercept = 0.05, linetype = "dashed", alpha=0.5) +
  geom_point() +
  geom_line() +
  scale_color_manual(values = group_colors) +
  scale_shape_manual(values = group_shapes) +
  ylab("Probability of p < 0.05") +
  xlab("Difference at Time0 (in standard deviation units)") +
  theme_pubr() +
  theme(legend.position="bottom") +
  guides(col = guide_legend(ncol = 5)) +
  facet_grid(.~effect_model, labeller = "label_both") +
  NULL
gg1



```

